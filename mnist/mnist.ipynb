{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.00000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.456643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219286</td>\n",
       "      <td>0.117095</td>\n",
       "      <td>0.059024</td>\n",
       "      <td>0.02019</td>\n",
       "      <td>0.017238</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.887730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.312890</td>\n",
       "      <td>4.633819</td>\n",
       "      <td>3.274488</td>\n",
       "      <td>1.75987</td>\n",
       "      <td>1.894498</td>\n",
       "      <td>0.414264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>253.00000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              label   pixel0   pixel1   pixel2   pixel3   pixel4   pixel5  \\\n",
       "count  42000.000000  42000.0  42000.0  42000.0  42000.0  42000.0  42000.0   \n",
       "mean       4.456643      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "std        2.887730      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "min        0.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "25%        2.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "50%        4.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "75%        7.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "max        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "        pixel6   pixel7   pixel8    ...         pixel774      pixel775  \\\n",
       "count  42000.0  42000.0  42000.0    ...     42000.000000  42000.000000   \n",
       "mean       0.0      0.0      0.0    ...         0.219286      0.117095   \n",
       "std        0.0      0.0      0.0    ...         6.312890      4.633819   \n",
       "min        0.0      0.0      0.0    ...         0.000000      0.000000   \n",
       "25%        0.0      0.0      0.0    ...         0.000000      0.000000   \n",
       "50%        0.0      0.0      0.0    ...         0.000000      0.000000   \n",
       "75%        0.0      0.0      0.0    ...         0.000000      0.000000   \n",
       "max        0.0      0.0      0.0    ...       254.000000    254.000000   \n",
       "\n",
       "           pixel776     pixel777      pixel778      pixel779  pixel780  \\\n",
       "count  42000.000000  42000.00000  42000.000000  42000.000000   42000.0   \n",
       "mean       0.059024      0.02019      0.017238      0.002857       0.0   \n",
       "std        3.274488      1.75987      1.894498      0.414264       0.0   \n",
       "min        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "25%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "50%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "75%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "max      253.000000    253.00000    254.000000     62.000000       0.0   \n",
       "\n",
       "       pixel781  pixel782  pixel783  \n",
       "count   42000.0   42000.0   42000.0  \n",
       "mean        0.0       0.0       0.0  \n",
       "std         0.0       0.0       0.0  \n",
       "min         0.0       0.0       0.0  \n",
       "25%         0.0       0.0       0.0  \n",
       "50%         0.0       0.0       0.0  \n",
       "75%         0.0       0.0       0.0  \n",
       "max         0.0       0.0       0.0  \n",
       "\n",
       "[8 rows x 785 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('label',axis=1)\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "from tensorflow.python.framework import ops\n",
    "label_binarizer = sklearn.preprocessing.LabelBinarizer()\n",
    "label_binarizer.fit(range(10))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "X_train = X_train.T/255\n",
    "X_test = X_test.T/255\n",
    "y_train = label_binarizer.transform(y_train)\n",
    "y_test = label_binarizer.transform(y_test)\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_placeholder(n_x,n_y):\n",
    "    X = tf.placeholder(dtype=tf.float32,shape=[n_x,None],name='X')\n",
    "    Y = tf.placeholder(dtype=tf.float32,shape=[n_y,None,],name='Y')\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    parameters={}\n",
    "    for i in range(len(layer_dims)-1):\n",
    "        parameters['W'+str(i+1)] = tf.get_variable('W'+str(i+1),shape = [layer_dims[i+1],layer_dims[i]],\n",
    "                                                   dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        parameters['b'+str(i+1)] = tf.get_variable('b'+str(i+1),shape=[layer_dims[i+1],1],initializer=tf.zeros_initializer())\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X,parameters):\n",
    "    L = len(parameters)//2\n",
    "    X_prev = X\n",
    "    for i in np.arange(L-1):\n",
    "        Z = tf.add(tf.matmul(parameters['W'+str(i+1)],X_prev),parameters['b'+str(i+1)])\n",
    "        X = tf.nn.relu(Z)\n",
    "        X_prev = X\n",
    "    ZL = tf.add(tf.matmul(parameters['W'+str(L)],X),parameters['b'+str(L)])\n",
    "    return ZL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z,y):\n",
    "    logits = tf.transpose(Z)\n",
    "    labels = tf.transpose(y)\n",
    "    cost = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels)\n",
    "    return tf.reduce_mean(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_min_batches(X_train,Y_train,minibatch_size,seed):\n",
    "    m = X_train.shape[1]\n",
    "    np.random.seed(seed)\n",
    "    index = range(m)\n",
    "    np.random.shuffle(index)\n",
    "    X_train = X_train.iloc[:,index]\n",
    "    Y_train = Y_train[:,index]\n",
    "    num = int(m/minibatch_size)\n",
    "    cur = 0\n",
    "    mini_batches = []\n",
    "    for i in range(num):\n",
    "        minibatch_X = X_train.iloc[:,cur:(cur+minibatch_size)]\n",
    "        minibatch_Y = Y_train[:,cur:cur+minibatch_size]\n",
    "        cur += minibatch_size\n",
    "        mini_batches.append((minibatch_X,minibatch_Y))\n",
    "    minibatch_X=X_train.iloc[:,cur:]\n",
    "    minibatch_Y=Y_train[:,cur:]\n",
    "    mini_batches.append((minibatch_X,minibatch_Y))\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 400, minibatch_size = 32, print_cost = True,layer_dims=[784,30,10]):\n",
    "    tf.reset_default_graph()\n",
    "    costs = []\n",
    "    seed = 3\n",
    "    nx = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "    ny = 10\n",
    "    X,Y=get_placeholder(nx,ny)\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    Z = forward_propagation(X,parameters)\n",
    "    cost = compute_cost(Z,Y)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    correct_prediction = tf.equal(tf.argmax(Z), tf.argmax(Y))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,'float'))\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "            seed = seed + 1\n",
    "            epoch_cost = 0.    \n",
    "            mini_batches = random_min_batches(X_train,Y_train,minibatch_size,seed)\n",
    "            for mini_batch in mini_batches:\n",
    "                (minibatch_X,minibatch_Y) = mini_batch\n",
    "                _ , minibatch_cost = sess.run([optimizer,cost], feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "                epoch_cost += minibatch_cost/int(m/minibatch_size)\n",
    "\n",
    "            if print_cost == True and epoch%20 == 0:\n",
    "                print('cost after iteration %i %f'%(epoch,epoch_cost))\n",
    "                print('train accacy ',accuracy.eval({X:X_train,Y:Y_train}))\n",
    "                print('test accucy',accuracy.eval({X:X_test,Y:Y_test}))\n",
    "            if print_cost == True and epoch%5==0:\n",
    "                costs.append(epoch_cost)\n",
    "\n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        parameters = sess.run(parameters)\n",
    "\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after iteration 0 2.339454\n",
      "('train accacy ', 0.090646259)\n",
      "('test accucy', 0.084285714)\n",
      "cost after iteration 20 1.365885\n",
      "('train accacy ', 0.69649661)\n",
      "('test accucy', 0.70912701)\n",
      "cost after iteration 40 0.820305\n",
      "('train accacy ', 0.80105442)\n",
      "('test accucy', 0.8125397)\n",
      "cost after iteration 60 0.611819\n",
      "('train accacy ', 0.84217685)\n",
      "('test accucy', 0.84952378)\n",
      "cost after iteration 80 0.509947\n",
      "('train accacy ', 0.86418366)\n",
      "('test accucy', 0.87277776)\n",
      "cost after iteration 100 0.449284\n",
      "('train accacy ', 0.87850338)\n",
      "('test accucy', 0.88507938)\n",
      "cost after iteration 120 0.408672\n",
      "('train accacy ', 0.88717687)\n",
      "('test accucy', 0.89174604)\n",
      "cost after iteration 140 0.379653\n",
      "('train accacy ', 0.89394557)\n",
      "('test accucy', 0.8976984)\n",
      "cost after iteration 160 0.357679\n",
      "('train accacy ', 0.89918369)\n",
      "('test accucy', 0.90238094)\n",
      "cost after iteration 180 0.340420\n",
      "('train accacy ', 0.90357143)\n",
      "('test accucy', 0.90531749)\n",
      "cost after iteration 200 0.326274\n",
      "('train accacy ', 0.90809524)\n",
      "('test accucy', 0.90809524)\n",
      "cost after iteration 220 0.314388\n",
      "('train accacy ', 0.91129249)\n",
      "('test accucy', 0.91007936)\n",
      "cost after iteration 240 0.304164\n",
      "('train accacy ', 0.91445577)\n",
      "('test accucy', 0.91111112)\n",
      "cost after iteration 260 0.295178\n",
      "('train accacy ', 0.91710883)\n",
      "('test accucy', 0.91317463)\n",
      "cost after iteration 280 0.287050\n",
      "('train accacy ', 0.9194898)\n",
      "('test accucy', 0.91484129)\n",
      "cost after iteration 300 0.279745\n",
      "('train accacy ', 0.92176872)\n",
      "('test accucy', 0.9172222)\n",
      "cost after iteration 320 0.273028\n",
      "('train accacy ', 0.92346936)\n",
      "('test accucy', 0.91904759)\n",
      "cost after iteration 340 0.266827\n",
      "('train accacy ', 0.92493194)\n",
      "('test accucy', 0.9204762)\n",
      "cost after iteration 360 0.261074\n",
      "('train accacy ', 0.92673469)\n",
      "('test accucy', 0.92182541)\n",
      "cost after iteration 380 0.255598\n",
      "('train accacy ', 0.92819726)\n",
      "('test accucy', 0.92317462)\n",
      "cost after iteration 400 0.250523\n",
      "('train accacy ', 0.92942178)\n",
      "('test accucy', 0.92420638)\n",
      "cost after iteration 420 0.245719\n",
      "('train accacy ', 0.93071431)\n",
      "('test accucy', 0.92603177)\n",
      "cost after iteration 440 0.241071\n",
      "('train accacy ', 0.93159866)\n",
      "('test accucy', 0.92722225)\n",
      "cost after iteration 460 0.236669\n",
      "('train accacy ', 0.93299317)\n",
      "('test accucy', 0.92849207)\n",
      "cost after iteration 480 0.232436\n",
      "('train accacy ', 0.93404764)\n",
      "('test accucy', 0.92960316)\n",
      "cost after iteration 500 0.228312\n",
      "('train accacy ', 0.93513608)\n",
      "('test accucy', 0.93015873)\n",
      "cost after iteration 520 0.224379\n",
      "('train accacy ', 0.93605441)\n",
      "('test accucy', 0.9311111)\n",
      "cost after iteration 540 0.220655\n",
      "('train accacy ', 0.93687075)\n",
      "('test accucy', 0.93198413)\n",
      "cost after iteration 560 0.216922\n",
      "('train accacy ', 0.93792516)\n",
      "('test accucy', 0.93293649)\n",
      "cost after iteration 580 0.213350\n",
      "('train accacy ', 0.93887752)\n",
      "('test accucy', 0.93404764)\n",
      "cost after iteration 600 0.209876\n",
      "('train accacy ', 0.93986392)\n",
      "('test accucy', 0.93460315)\n",
      "cost after iteration 620 0.206595\n",
      "('train accacy ', 0.94102043)\n",
      "('test accucy', 0.93515873)\n",
      "cost after iteration 640 0.203264\n",
      "('train accacy ', 0.94214284)\n",
      "('test accucy', 0.9357143)\n",
      "cost after iteration 660 0.200013\n",
      "('train accacy ', 0.9430272)\n",
      "('test accucy', 0.93642855)\n",
      "cost after iteration 680 0.196931\n",
      "('train accacy ', 0.94418365)\n",
      "('test accucy', 0.93730158)\n",
      "cost after iteration 700 0.193963\n",
      "('train accacy ', 0.94520408)\n",
      "('test accucy', 0.93801588)\n",
      "cost after iteration 720 0.190977\n",
      "('train accacy ', 0.94591838)\n",
      "('test accucy', 0.93888891)\n",
      "cost after iteration 740 0.188147\n",
      "('train accacy ', 0.9467687)\n",
      "('test accucy', 0.93960315)\n",
      "cost after iteration 760 0.185335\n",
      "('train accacy ', 0.94768709)\n",
      "('test accucy', 0.94047618)\n",
      "cost after iteration 780 0.182629\n",
      "('train accacy ', 0.94860542)\n",
      "('test accucy', 0.94111109)\n",
      "cost after iteration 800 0.179969\n",
      "('train accacy ', 0.94955784)\n",
      "('test accucy', 0.94142854)\n",
      "cost after iteration 820 0.177396\n",
      "('train accacy ', 0.94989794)\n",
      "('test accucy', 0.94206351)\n",
      "cost after iteration 840 0.174820\n",
      "('train accacy ', 0.9505102)\n",
      "('test accucy', 0.94261903)\n",
      "cost after iteration 860 0.172394\n",
      "('train accacy ', 0.95119047)\n",
      "('test accucy', 0.94301587)\n",
      "cost after iteration 880 0.170010\n",
      "('train accacy ', 0.95193875)\n",
      "('test accucy', 0.9438889)\n",
      "cost after iteration 900 0.167591\n",
      "('train accacy ', 0.952483)\n",
      "('test accucy', 0.94460315)\n",
      "cost after iteration 920 0.165259\n",
      "('train accacy ', 0.95329934)\n",
      "('test accucy', 0.94499999)\n",
      "cost after iteration 940 0.163013\n",
      "('train accacy ', 0.95397961)\n",
      "('test accucy', 0.94531745)\n",
      "cost after iteration 960 0.160807\n",
      "('train accacy ', 0.95455784)\n",
      "('test accucy', 0.94595236)\n",
      "cost after iteration 980 0.158672\n",
      "('train accacy ', 0.95510203)\n",
      "('test accucy', 0.94611108)\n",
      "cost after iteration 1000 0.156626\n",
      "('train accacy ', 0.95578229)\n",
      "('test accucy', 0.94674605)\n",
      "cost after iteration 1020 0.154649\n",
      "('train accacy ', 0.9562245)\n",
      "('test accucy', 0.94730157)\n",
      "cost after iteration 1040 0.152486\n",
      "('train accacy ', 0.9572109)\n",
      "('test accucy', 0.94825399)\n",
      "cost after iteration 1060 0.150489\n",
      "('train accacy ', 0.95768708)\n",
      "('test accucy', 0.94825399)\n",
      "cost after iteration 1080 0.148555\n",
      "('train accacy ', 0.95833331)\n",
      "('test accucy', 0.94857144)\n",
      "cost after iteration 1100 0.146666\n",
      "('train accacy ', 0.95887756)\n",
      "('test accucy', 0.9488095)\n",
      "cost after iteration 1120 0.144845\n",
      "('train accacy ', 0.95931971)\n",
      "('test accucy', 0.94912696)\n",
      "cost after iteration 1140 0.142967\n",
      "('train accacy ', 0.95986396)\n",
      "('test accucy', 0.94936508)\n",
      "cost after iteration 1160 0.141133\n",
      "('train accacy ', 0.96030611)\n",
      "('test accucy', 0.94984126)\n",
      "cost after iteration 1180 0.139373\n",
      "('train accacy ', 0.96064627)\n",
      "('test accucy', 0.95023811)\n",
      "cost after iteration 1200 0.137630\n",
      "('train accacy ', 0.9612245)\n",
      "('test accucy', 0.95095241)\n",
      "cost after iteration 1220 0.135932\n",
      "('train accacy ', 0.96153063)\n",
      "('test accucy', 0.95126987)\n",
      "cost after iteration 1240 0.134262\n",
      "('train accacy ', 0.96200681)\n",
      "('test accucy', 0.95150793)\n",
      "cost after iteration 1260 0.132634\n",
      "('train accacy ', 0.96282315)\n",
      "('test accucy', 0.9520635)\n",
      "cost after iteration 1280 0.131029\n",
      "('train accacy ', 0.9632653)\n",
      "('test accucy', 0.95253968)\n",
      "cost after iteration 1300 0.129444\n",
      "('train accacy ', 0.96391159)\n",
      "('test accucy', 0.95261908)\n",
      "cost after iteration 1320 0.127921\n",
      "('train accacy ', 0.96448982)\n",
      "('test accucy', 0.95325398)\n",
      "cost after iteration 1340 0.126370\n",
      "('train accacy ', 0.96503401)\n",
      "('test accucy', 0.95333332)\n",
      "cost after iteration 1360 0.124845\n",
      "('train accacy ', 0.96551019)\n",
      "('test accucy', 0.95349205)\n",
      "cost after iteration 1380 0.123387\n",
      "('train accacy ', 0.96608841)\n",
      "('test accucy', 0.95373017)\n",
      "cost after iteration 1400 0.121915\n",
      "('train accacy ', 0.96666664)\n",
      "('test accucy', 0.95357144)\n",
      "cost after iteration 1420 0.120518\n",
      "('train accacy ', 0.9670068)\n",
      "('test accucy', 0.95396823)\n",
      "cost after iteration 1440 0.119121\n",
      "('train accacy ', 0.96741498)\n",
      "('test accucy', 0.95412701)\n",
      "cost after iteration 1460 0.117751\n",
      "('train accacy ', 0.96778911)\n",
      "('test accucy', 0.95420635)\n",
      "cost after iteration 1480 0.116360\n",
      "('train accacy ', 0.96812928)\n",
      "('test accucy', 0.95428574)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHIWCiIjk/DcHB37PDbQVzgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fde88fcbd50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = model(X_train, y_train, X_test, y_test,layer_dims=[784,100,30,10],num_epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-247-b6492f8aa398>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'parameters' is not defined"
     ]
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
