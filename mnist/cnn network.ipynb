{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_data = data.drop('label',axis=1)\n",
    "y_data = data['label']\n",
    "X_data = X_data/255.0\n",
    "y_data = y_data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data,y_data,test_size=0.2, random_state=0)\n",
    "X_matrix=X_train.as_matrix()\n",
    "X_train = X_matrix.reshape(X_matrix.shape[0],28,28,1)\n",
    "X_matrix=X_test.as_matrix()\n",
    "X_test = X_matrix.reshape(X_matrix.shape[0],28,28,1)\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "PIXEL_DEPTH = 255\n",
    "BATCH_SIZE = 128\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = X_train.shape[0]\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = X_test.shape[0]\n",
    "NUM_CLASSES = 10\n",
    "SEED = 66478 \n",
    "NUM_EPOCHS = 10\n",
    "EVAL_FREQUENCY = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(predictions, labels):\n",
    "  \"\"\"Return the error rate based on dense predictions and sparse labels.\"\"\"\n",
    "  return 100.0 - (\n",
    "      100.0 *\n",
    "      numpy.sum(numpy.argmax(predictions, 1) == labels) /\n",
    "      predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy\n",
    "import sys\n",
    "from tensorflow.python.framework import ops\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    error_mini=[]\n",
    "    ops.reset_default_graph() \n",
    "    train_data_node = tf.placeholder(tf.float32,shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "    train_label_node = tf.placeholder(tf.int32,shape=(BATCH_SIZE,))\n",
    "    eval_data = tf.placeholder(tf.float32,shape=(NUM_EXAMPLES_PER_EPOCH_FOR_EVAL,IMAGE_SIZE,IMAGE_SIZE,NUM_CHANNELS))\n",
    "    conv1_weights = tf.Variable(tf.truncated_normal([5, 5, NUM_CHANNELS, 32],stddev=0.1,seed=SEED, dtype=tf.float32))\n",
    "    conv1_biases = tf.Variable(tf.zeros([32], dtype=tf.float32))\n",
    "    conv2_weights = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1,seed=SEED, dtype=tf.float32))\n",
    "    conv2_biases = tf.Variable(tf.constant(0.1, shape=[64], dtype=tf.float32))\n",
    "    fc1_weights = tf.Variable(tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n",
    "                                                  stddev=0.1,seed=SEED,dtype=tf.float32))\n",
    "    fc1_biases = tf.Variable(tf.constant(0.1, shape=[512], dtype=tf.float32))\n",
    "    fc2_weights = tf.Variable(tf.truncated_normal([512, NUM_CLASSES],\n",
    "                                                    stddev=0.1,\n",
    "                                                    seed=SEED,\n",
    "                                                    dtype=tf.float32))\n",
    "    fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_CLASSES], dtype=tf.float32))\n",
    "    num_epochs = NUM_EPOCHS\n",
    "    train_size = X_train.shape[0]\n",
    "\n",
    "    def model(data, train=False):\n",
    "        \"\"\"The Model definition.\"\"\"\n",
    "        # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "        # the same size as the input). Note that {strides} is a 4D array whose\n",
    "        # shape matches the data layout: [image index, y, x, depth].\n",
    "        conv = tf.nn.conv2d(data,\n",
    "                            conv1_weights,\n",
    "                            strides=[1, 1, 1, 1],\n",
    "                            padding='SAME')\n",
    "        # Bias and rectified linear non-linearity.\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "        # Max pooling. The kernel size spec {ksize} also follows the layout of\n",
    "        # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "        pool = tf.nn.max_pool(relu,\n",
    "                              ksize=[1, 2, 2, 1],\n",
    "                              strides=[1, 2, 2, 1],\n",
    "                              padding='SAME')\n",
    "        conv = tf.nn.conv2d(pool,\n",
    "                            conv2_weights,\n",
    "                            strides=[1, 1, 1, 1],\n",
    "                            padding='SAME')\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "        pool = tf.nn.max_pool(relu,\n",
    "                              ksize=[1, 2, 2, 1],\n",
    "                              strides=[1, 2, 2, 1],\n",
    "                              padding='SAME')\n",
    "        # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "        # fully connected layers.\n",
    "        pool_shape = pool.get_shape().as_list()\n",
    "        reshape = tf.reshape(\n",
    "            pool,\n",
    "            [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "        # Fully connected layer. Note that the '+' operation automatically\n",
    "        # broadcasts the biases.\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "        # Add a 50% dropout during training only. Dropout also scales\n",
    "        # activations such that no rescaling is needed at evaluation time.\n",
    "        if train:\n",
    "          hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "        return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "      # Training computation: logits + cross-entropy loss.\n",
    "    logits = model(train_data_node, True)\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      logits=logits, labels=train_label_node))\n",
    "\n",
    "    # L2 regularization for the fully connected parameters.\n",
    "    regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                  tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "    # Add the regularization term to the loss.\n",
    "    loss += 5e-4 * regularizers\n",
    "\n",
    "    # Optimizer: set up a variable that's incremented once per batch and\n",
    "    # controls the learning rate decay.\n",
    "    batch = tf.Variable(0, dtype=tf.float32)\n",
    "    # Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        0.01,                # Base learning rate.\n",
    "        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "        train_size,          # Decay step.\n",
    "        0.95,                # Decay rate.\n",
    "        staircase=True)\n",
    "    # Use simple momentum for the optimization.\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate,0.9).minimize(loss,global_step=batch)\n",
    "    # Predictions for the current training minibatch.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Predictions for the test and validation, which we'll compute less often.\n",
    "    eval_prediction = tf.nn.softmax(model(eval_data))\n",
    "    #  Create a local session to run the training.\n",
    "    start_time = time.time()\n",
    "    with tf.Session() as sess:\n",
    "        # Run all the initializers to prepare the trainable parameters.\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        print('Initialized!')\n",
    "        # Loop through training steps.\n",
    "        for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\n",
    "              # Compute the offset of the current minibatch in the data.\n",
    "              # Note that we could use better randomization across epochs.\n",
    "            offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "            batch_data = X_train[offset:(offset + BATCH_SIZE),:]\n",
    "            batch_labels = y_train[offset:(offset + BATCH_SIZE)]\n",
    "            # This dictionary maps the batch data (as a numpy array) to the\n",
    "            # node in the graph it should be fed to.\n",
    "            feed_dict = {train_data_node: batch_data,\n",
    "                         train_label_node: batch_labels}\n",
    "            # Run the optimizer to update weights.\n",
    "            sess.run(optimizer, feed_dict=feed_dict)\n",
    "            # print some extra information once reach the evaluation frequency\n",
    "            if step % EVAL_FREQUENCY == 0:\n",
    "                \n",
    "              # fetch some extra nodes' data\n",
    "                l, lr, predictions = sess.run([loss, learning_rate, train_prediction],\n",
    "                                            feed_dict=feed_dict)\n",
    "                elapsed_time = time.time() - start_time\n",
    "                start_time = time.time()\n",
    "                print('Step %d (epoch %.2f), %.1f ms' %\n",
    "                      (step, float(step) * BATCH_SIZE / train_size,\n",
    "                       1000 * elapsed_time /EVAL_FREQUENCY))\n",
    "                print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "                print('Minibatch error: %.1f%%' % error_rate(predictions, batch_labels))\n",
    "                error_mini.append(error_rate(predictions, batch_labels))\n",
    "        plt.plot(numpy.squeeze(error_mini))\n",
    "        plt.ylabel('error mini')\n",
    "        plt.xlabel('iterations (per hundreds)')\n",
    "        plt.show()\n",
    "        test_pr = sess.run(eval_prediction,feed_dict={ eval_data:X_test})\n",
    "        print('eval error: %.1f%%'%error_rate(test_pr,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33600, 28, 28, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "Step 0 (epoch 0.00), 4.9 ms\n",
      "Minibatch loss: 7.826, learning rate: 0.010000\n",
      "Minibatch error: 84.4%\n",
      "Step 100 (epoch 0.38), 601.1 ms\n",
      "Minibatch loss: 3.365, learning rate: 0.010000\n",
      "Minibatch error: 8.6%\n",
      "Step 200 (epoch 0.76), 582.1 ms\n",
      "Minibatch loss: 3.265, learning rate: 0.010000\n",
      "Minibatch error: 6.2%\n",
      "Step 300 (epoch 1.14), 654.8 ms\n",
      "Minibatch loss: 3.122, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Step 400 (epoch 1.52), 694.5 ms\n",
      "Minibatch loss: 3.195, learning rate: 0.009500\n",
      "Minibatch error: 7.0%\n",
      "Step 500 (epoch 1.90), 682.6 ms\n",
      "Minibatch loss: 3.177, learning rate: 0.009500\n",
      "Minibatch error: 2.3%\n",
      "Step 600 (epoch 2.29), 674.0 ms\n",
      "Minibatch loss: 3.026, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Step 700 (epoch 2.67), 756.7 ms\n",
      "Minibatch loss: 2.977, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Step 800 (epoch 3.05), 770.6 ms\n",
      "Minibatch loss: 2.933, learning rate: 0.008574\n",
      "Minibatch error: 2.3%\n",
      "Step 900 (epoch 3.43), 739.7 ms\n",
      "Minibatch loss: 3.088, learning rate: 0.008574\n",
      "Minibatch error: 5.5%\n",
      "Step 1000 (epoch 3.81), 916.7 ms\n",
      "Minibatch loss: 2.865, learning rate: 0.008574\n",
      "Minibatch error: 0.8%\n",
      "Step 1100 (epoch 4.19), 1007.7 ms\n",
      "Minibatch loss: 2.865, learning rate: 0.008145\n",
      "Minibatch error: 1.6%\n",
      "Step 1200 (epoch 4.57), 739.1 ms\n",
      "Minibatch loss: 2.857, learning rate: 0.008145\n",
      "Minibatch error: 3.9%\n",
      "Step 1300 (epoch 4.95), 757.8 ms\n",
      "Minibatch loss: 2.857, learning rate: 0.008145\n",
      "Minibatch error: 3.9%\n",
      "Step 1400 (epoch 5.33), 860.3 ms\n",
      "Minibatch loss: 2.833, learning rate: 0.007738\n",
      "Minibatch error: 3.9%\n",
      "Step 1500 (epoch 5.71), 782.4 ms\n",
      "Minibatch loss: 2.796, learning rate: 0.007738\n",
      "Minibatch error: 0.8%\n",
      "Step 1600 (epoch 6.10), 800.8 ms\n",
      "Minibatch loss: 2.738, learning rate: 0.007351\n",
      "Minibatch error: 0.8%\n",
      "Step 1700 (epoch 6.48), 787.1 ms\n",
      "Minibatch loss: 2.730, learning rate: 0.007351\n",
      "Minibatch error: 1.6%\n",
      "Step 1800 (epoch 6.86), 830.4 ms\n",
      "Minibatch loss: 2.709, learning rate: 0.007351\n",
      "Minibatch error: 1.6%\n",
      "Step 1900 (epoch 7.24), 902.9 ms\n",
      "Minibatch loss: 2.729, learning rate: 0.006983\n",
      "Minibatch error: 2.3%\n",
      "Step 2000 (epoch 7.62), 839.3 ms\n",
      "Minibatch loss: 2.677, learning rate: 0.006983\n",
      "Minibatch error: 0.8%\n",
      "Step 2100 (epoch 8.00), 657.7 ms\n",
      "Minibatch loss: 2.665, learning rate: 0.006634\n",
      "Minibatch error: 0.8%\n",
      "Step 2200 (epoch 8.38), 655.3 ms\n",
      "Minibatch loss: 2.625, learning rate: 0.006634\n",
      "Minibatch error: 0.8%\n",
      "Step 2300 (epoch 8.76), 661.2 ms\n",
      "Minibatch loss: 2.629, learning rate: 0.006634\n",
      "Minibatch error: 1.6%\n",
      "Step 2400 (epoch 9.14), 548.9 ms\n",
      "Minibatch loss: 2.598, learning rate: 0.006302\n",
      "Minibatch error: 0.8%\n",
      "Step 2500 (epoch 9.52), 550.8 ms\n",
      "Minibatch loss: 2.561, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Step 2600 (epoch 9.90), 586.5 ms\n",
      "Minibatch loss: 2.615, learning rate: 0.006302\n",
      "Minibatch error: 2.3%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f03e0e55d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval error: 1.4%\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
